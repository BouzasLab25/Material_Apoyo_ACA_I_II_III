{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Modelo Decision Field Theory de Busemeyer\"\n",
        "author: \n",
        "    - name: \"Christian Badillo.\"\n",
        "order: 1\n",
        "date: 2024-11-26\n",
        "date-format: full\n",
        "format: html\n",
        "bibilography: /DifussionModels/refDFT.bib\n",
        "nocite: |\n",
        "  @*\n",
        "toc-depth: 4\n",
        "---"
      ],
      "id": "0cb71e47"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El modelo ***Decision Field Theory*** de Busemeyer (1992, 1993) surge como a una alternativa a los modelos de decisión clásicos en el estudio de toma de decisión bajo riesgo e incertidumbre. Su objetivo es modelar el proceso de **deliberación** que lleva a la toma de decisiones, en lugar de simplemente describir el resultado de la decisión. Para ello se busca modelar dos fenómenos ampliamente documentados en la literatura de toma de decisiones.\n",
        "\n",
        "1. Inconsistencia en las preferencias. \n",
        "2. El tiempo disponible para decidir afecta la elección.\n",
        "\n",
        "El primer punto se refiere a que la transitividad entre las preferencias no siempre se cumple, es decir, si una persona prefiere A a B y B a C, no necesariamente preferirá A a C. El segundo punto da cuenta que la probabilidad de elección entre varía conforme pasa el tiempo, dado que se va acumulando evidencia a favor de una opción causando que con menor cantidad de tiempo se elija una alternativa que podría no ser la preferida con mayor tiempo de deliberación.\n",
        "\n",
        "## Construcción del Modelo\n",
        "\n",
        "Para constuir el modelo Busemyer en su artículo *Decision field theory: A dynamic-cognitive approach to decision making in an uncertain environment* (1993) da una serie de bloques que se añaden para crear el modelo basandose en modelos previos de toma de decisiones. En esta sección se resumira la construcción del modelo.\n",
        "\n",
        "![Construcción del modelo Decision Field Theory. Obtenida de Busemeyer y Townsend (1993).](/DifussionModels/img/model_build_busemeyer.png){width=100% #fig-fig1}\n",
        "\n",
        "### Modelos de Utilidad Esperada Subjetiva\n",
        "\n",
        "#### Utilidad Esperada Subjetiva (SEU).\n",
        "\n",
        "Las primeras tres etapas se basan en un modelo de utilidad esperada subjetiva (SEU, por sus siglas en inglés) que se utiliza para calcular la utilidad de cada opción. La utilidad se calcula al ponderar el valor de cada acción o decisión por la probabilidad subjetiva de que ocurra, este factor de ponderación igualmente se puede interpretar como la cantidad de atención que se le da a cada opción.\n",
        "\n",
        "Para ejemplificar el modelo se supondrá que se tienen dos opciones las cuales pueden conllevar a dos resultados diferentes cada uno con cierta probabilidad y cada posible resultado tendrá un valor. Se denotará las distintas acciones como A y B, sus posibles resultados como subíndices 1 y 2, y las probabilidades subjetivas de que ocurran como $w(S_1)$ y $w(S_2)$, respectivamente. La utilidad de cada resultado se denotará como $u(A_1)$, $u(A_2)$, $u(B_1)$ y $u(B_2)$. Entonces el utilidad esperada subjetiva ($v$) de cada elección se puede expresar como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "v_A = w(S_1) \\cdot u(A_1) +  w(S_2) \\cdot u(A_2)\\\\\n",
        "v_B = w(S_1) \\cdot u(B_1) +  w(S_2) \\cdot u(B_2)\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "La elección entre las dos opciones se basa en la diferencia de utilidad entre las dos opciones, es decir, la opción con mayor utilidad esperada subjetiva será la elegida, matemáticamente esto se expresa como una diferencia de utilidad:\n",
        "\n",
        "$$\n",
        "d = v_A - v_B\n",
        "$$\n",
        "\n",
        "Y la regla para decidir es:\n",
        "\n",
        "\\begin{equation}\n",
        "Decisión =\n",
        "\\begin{cases}\n",
        "A & \\text{si } d > 0\\\\\n",
        "B & \\text{si } d < 0\\\\\n",
        "\\text{Indiferencia} & \\text{si } d = 0\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "#### Utilidad Esperada Subjetiva Aleatoria (RSEU).\n",
        "\n",
        "El modelo SEU supone que la atención a cada opción es igual para cada presentación del mismo par de opciones. El modelo RSEU relaja esta suposición y permite que la atención a cada opción varíe en cada presentación, la diferencia $d$ en cada ensayo es aleatoria y es llamada **valencia diferencial** (*valence difference*). En este modelo el peso atencional $W(S_i)$ es una variable aleatoria que varía de ensayo a ensayo, representando así la flutuación en la atención.\n",
        "\n",
        "Como consecuencia la utilidad de cada opción se convierte en una variable aleatoria que a partir de ahora se le llamará como **valencia** ($V$). La cual se cálcula de firma idéntica al modelo SEU. La diferencia entre las valencias ahora se nombra como **estado de preferencia** $P$, para cada ensayo y se cálcula como:\n",
        "\n",
        "$$\n",
        "P = V_A - V_B = d + \\epsilon\n",
        "$$\n",
        "\n",
        "Donde $\\epsilon$ es un término de ruido que sigue una distribución de probabilidad, este término es el que permite que la elección varíe en cada ensayo. Por tanto, la probabilidad de elegir A sobre B se puede expresar como:\n",
        "\n",
        "$$\n",
        "Pr(A, B) = Pr(P > 0) = Pr(\\epsilon > -d)\n",
        "$$\n",
        "\n",
        "En muchos modelos de este estilo se supone que $\\epsilon$ sigue una distribución normal con media 0 y varianza $\\sigma^2$. Por tanto, la probabilidad de elección se puede expresar como:\n",
        "\n",
        "$$\n",
        "Pr(A, B) = Pr(\\epsilon > -d) = \\Phi\\left(\\frac{d}{\\sigma}\\right)\n",
        "$$\n",
        "\n",
        "Donde $\\Phi$ es la función de distribución acumulada de la distribución normal estándar. La cuál se puede visualizar en la siguiente gráfica para distintos valores de $\\sigma$ y $d$.\n"
      ],
      "id": "7fcf007f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import pandas as pd\n",
        "pio.renderers.default = \"iframe\"\n",
        "\n",
        "sigmas = [0.1, 0.5, 1, 2, 5]\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "y = [stats.norm.cdf(x/sigma, 0, 1) for sigma in sigmas]\n",
        "\n",
        "dic_y_data = {f\"{sigma}\": y_data for sigma, y_data in zip(sigmas, y)}\n",
        "df = pd.DataFrame(dic_y_data)\n",
        "df[\"d\"] = x\n",
        "\n",
        "fig = px.line(df, x=\"d\", y=df.columns[:-1], title=\"\", labels={\"variable\": \"σ\", \"value\": \"Pr(A, B)\"})\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"d\",\n",
        "    yaxis_title=\"Pr(A, B)\",\n",
        "    legend_title=\"σ\"\n",
        ")\n",
        "fig.show()"
      ],
      "id": "34f34e4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se observa la probabilidad ahora depende de la diferencia entre las valencias y la varianza de la distribución de ruido. A mayor varianza la probabilidad de elección se vuelve más aleatoria, mientras que a menor varianza la elección se vuelve más determinista. Este parámetro se toma en cuenta para modelar la variabilidad en la elección y es llamado **varianza de la valencia diferencial**.\n",
        "\n",
        "#### Utilidad Esperada Subjetiva Secuencial.\n",
        "\n",
        "El modelo SEU y RSEU suponen que la elección se realiza en un solo paso donde se calcula la utilidad de cada opción y se decide. Sin embargo, en la vida real esto no sucede dado que se toma tiempo en evaluar las opciones y se acumula evidencia a favor de una opción. Para modelar este proceso se añade una etapa de acumulación de evidencia a favor de una opción y se decide cuando la evidencia acumulada supera un umbral.\n",
        "\n",
        "Arreglemos las ecuaciones de arriba para tomar en cuenta la acumulación de evidencia. En primer ligar nuestro estado preferencial ya no es estático sino que varía en el tiempo, por tanto se puede expresar como:\n",
        "\n",
        "$$\n",
        "P(n) = P(n-1) + (V_A(n) - V_B(n)) + \\epsilon(n)\n",
        "$$\n",
        "\n",
        "Y nuestra regla de decisi es:\n",
        "\\begin{equation}\n",
        "Decisión =\n",
        "\\begin{cases}\n",
        "A & \\text{si } P(n) > \\theta\\\\\n",
        "B & \\text{si } P(n) < -\\theta\\\\\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "En el siguiente gif se muestra la acumulación de evidencia para distintos valores de $\\theta = 1$ y $\\sigma = 1$.\n",
        "\n",
        "![Modelo de Utilidad Esperada Subjetiva Secuencial](/DifussionModels/img/sequential_seu.gif){width=100% #fig-fig2}\n",
        "\n",
        "Se puede obtener la distribuvión de probabilidad de elejir A antes que B dado los valores de los parámetros $\\theta$, $\\sigma$ y $d$, la cual se puede expresar como:\n",
        "$$\n",
        "Pr(A, B) = Pr(P(n) > \\theta) = F \\left( 2 \\cdot \\frac{d}{\\sigma} \\cdot \\frac{\\theta}{\\sigma} \\right)\n",
        "$$\n",
        "\n",
        "En este caso $F$ representa la distribución logística acumulada. En las siguientes gráficas se muestra la probabilidad de elección para distintos valores de $\\theta$, $\\sigma$ y $d$.\n"
      ],
      "id": "99e2e385"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "thetas = [0.1, 0.5, 1, 2]\n",
        "sigmas = [0.1, 0.5, 1, 2]\n",
        "d = np.linspace(-5, 5, 1000)\n",
        "\n",
        "# Definir colores fijos para cada theta\n",
        "colors = [\"blue\", \"red\", \"green\", \"purple\"]\n",
        "\n",
        "# Calcular los valores de la función logística\n",
        "y = [[stats.logistic.cdf(2 * d / sigma * theta, loc=0, scale=1) for theta in thetas] for sigma in sigmas]\n",
        "\n",
        "# Crear subplots\n",
        "figs = make_subplots(rows=2, cols=2, subplot_titles=[f\"σ = {sigma}\" for sigma in sigmas])\n",
        "\n",
        "# Agregar trazas con hover funcional\n",
        "for j, sigma in enumerate(sigmas):\n",
        "    row, col = divmod(j, 2)  # Posición en la cuadrícula (0-based)\n",
        "    row += 1\n",
        "    col += 1\n",
        "\n",
        "    for i, theta in enumerate(thetas):\n",
        "        figs.add_trace(\n",
        "            go.Scatter(\n",
        "                x=d, \n",
        "                y=y[j][i], \n",
        "                mode=\"lines\", \n",
        "                name=f\"θ = {theta}\" if j == 0 else None,  # Solo mostrar la leyenda en el primer subplot\n",
        "                line=dict(color=colors[i]), \n",
        "                showlegend=(j == 0),  # Mostrar leyenda solo en la primera iteración\n",
        "                hovertemplate=\"<b>d:</b> %{x:.2f}<br><b>Pr(A, B):</b> %{y:.4f}<br><b>θ:</b> \" + str(theta) + \"<br><b>σ:</b> \" + str(sigma)\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "# Modificar etiquetas de los ejes por subplot\n",
        "for j, sigma in enumerate(sigmas):\n",
        "    row, col = divmod(j, 2)\n",
        "    row += 1\n",
        "    col += 1\n",
        "    figs.update_xaxes(title_text=f\"d\", row=row, col=col)\n",
        "    figs.update_yaxes(title_text=f\"Pr(A, B)\", row=row, col=col)\n",
        "\n",
        "# Configurar diseño general con hover mejorado\n",
        "figs.update_layout(\n",
        "    legend_title=\"θ\",\n",
        "    hovermode=\"x unified\" \n",
        ")\n",
        "figs.show()"
      ],
      "id": "14c592d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El tiempo necesario para tomar una decisión es aleatorio, dado que depende de la acumulación de evidencia y el ruido en el proceso. En su artículo Busemeyer y Townsend (1993) mencionan que el tiempo esperado o promedio para tomar una decisión se puede calcular como:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}(N) = \\left( \\frac{\\theta}{d} \\right) \\cdot (2 \\cdot Pr(A, B) - 1)\n",
        "$$\n",
        "\n",
        "Indicando que depende de la diferencia entre las valencias y el umbral de decisión. A mayor diferencia entre las valencias menor tiempo se necesita para tomar una decisión y a un umbral de decisión más grande mayor tiempo se necesita. Esto se conoce como un ***trade-off entre la velocidad y la precisión*** o entre **costo y beneficio** en la toma de decisiones, que en este modelo se puede ajustar con el umbral de decisión $\\theta$. Si una elección conlleva un costo bajo en equivocarse se puede ajustar el umbral a un valor bajo para tomar decisiones rápidas, mientras que si el costo de equivocarse es alto se puede ajustar el umbral a un valor alto para tomar decisiones más precisas.\n",
        "\n",
        "El siguiente paso en mostrado en la figura @fig-fig1 es lo que llaman **Random Walk SEU** que es justo lo que se muestra en el gif (véase figura @fig-fig2) donde se acumula evidencia a favor de una opción y se decide cuando se supera un umbral, pero se generaliza el punto de partida, ya que en el gif se parte del cero, pero podemos iniciar en cualquier punto. Si el punto de inicio es cercano al umbral de decisión de alguna de las opciones se dice que hay un **sesgo** en la elección. Este sesgo puede ser provocado por distintos factores como la preferencia por una opción, la cantidad de información que se tiene de una opción, entre otros.\n",
        "\n",
        "### Memoria\n",
        "\n",
        "Con las modificaciones creadas hasta ahora el modelo es capaz de representar el proceso de deliberación en la toma de decisiones bastante bien y es una plantilla ampliamente recreada en otros modelos similares. Lo que verdaderamente hace único al modelo de Busemeyer es la inclusión de la memoria en el proceso de toma de decisiones. La memoria se incluye para modelar la inconsistencia en las preferencias y el efecto del tiempo en la elección. Para lograrlo se añade un parámetro llamado **tasa de crecimiento/decaimiento** ($s$) que modela la aparición de efectos de recencia y primacía en la elección.\n",
        "\n",
        "La recencia se refiere a que la información más reciente tiene mayor peso en la elección, mientras que la primacía se refiere a que la información que se presenta primero tiene mayor peso en la elección. La tasa de crecimiento/decaimiento modela la rapidez con la que se olvida la información, si $0 < s < 1$ se presenta recencia, si $s < 0$ se presenta primacía y si $s = 0$ no hay efecto de memoria. Esto es posible dada la forma matemática en la que se añade la memoria al modelo.\n",
        "\n",
        "$$\n",
        "P(n) = (1 - s) \\cdot P(n-1) + (V_A(n) - V_B(n)) + \\epsilon(n)\n",
        "$$\n",
        "\n",
        "En los siguientes gifs se muestra el efecto de la memoria en la elección para un $s = 0.99$ (@fig-fig3) y $s = -3$ (@fig-fig4).\n",
        "\n",
        "![Efecto de Recencia.](/DifussionModels/img/linear_seu_s_low.gif){width=100% #fig-fig3}\n",
        "\n",
        "![Efecto de Primacia.](/DifussionModels/img/linear_seu_prima.gif){width=100% #fig-fig4}\n",
        "\n",
        "El efecto es apenas perceptible, pero se nota que principalmente se ve en el tiempo para decidir. En el caso de recencia se toma menos tiempo, mientras que en el caso de primacía se toma más tiempo. \n",
        "\n",
        "### Control aversivo y apetitivo: Decision Field Theory.\n",
        "\n",
        "La última modificación al modelo seda al incluir un parámetro $c$ llamado **gradiente objetivo**, dado que nos permite dar cuenta de que tan apetitiva son las opciones. Para valores de $c > 0$ se dice que las opciones son apetitivas, mientras que para valores de $c < 0$ se dice que las opciones son aversivas. Este parámetro se añade al proceso de acumulación de evidencia de la siguiente forma:\n",
        "\n",
        "$$\n",
        "P(n) = (1 - (s + c)) \\cdot P(n-1) + (V_A(n) - V_B(n)) + \\epsilon(n)\n",
        "$$\n",
        "\n",
        "Esta última ecuación representa el modelo completo de Busemeyer y Townsend (1993).\n",
        "\n",
        "## Versión Interactiva.\n",
        "\n",
        "En el siguiente enlace a Google Colab se puede interactuar con el modelo de Busemeyer y Townsend (1993) para ver el efecto de los parámetros en la elección y el tiempo de decisión.\n",
        "\n",
        "- [Google Colab: Modelo Decision Field Theory de Busemeyer](https://colab.research.google.com/drive/1JCeTkHNhcOXtT3AHQb6w2eocAJxngUgf?usp=sharing)\n",
        "\n",
        "## Referencias\n"
      ],
      "id": "43dd0898"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}