[
  {
    "objectID": "student_guide.html",
    "href": "student_guide.html",
    "title": "",
    "section": "",
    "text": "Ver código\n\n\n\n\n\n\n\nGuía de Estudio\n\n\n\nEn esta sección encontrarás material de apoyo (apuntes de estudiantes) para el estudio de los temas de la asignatura de Aprendizaje y Conducta Adaptable con el Dr. Arturo Bouzas.\n\n\n\n\n\n\n\n\n\n\n\nGuía de Estudio para Aprendizaje y Conducta Adaptativa 2\n\n\nPor María Nore Rodríguez González\n\n\n\n\n\nGuías por Temas\n\n\n\n\n\nAutor\n\n\nEnlace\n\n\nTema\n\n\n\n\n\n\nAutor 1\n\n\nTexto 1\n\n\nMateria A\n\n\n\n\nAutor 2\n\n\nTexto 2\n\n\nMateria B\n\n\n\n\nAutor 3\n\n\nTexto 3\n\n\nMateria C\n\n\n\n\nAutor 4\n\n\nTexto 4\n\n\nMateria A\n\n\n\n\nAutor 5\n\n\nTexto 5\n\n\nMateria D",
    "crumbs": [
      "Guías de Estudio"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje y Conducta Adaptable: Material de Apoyo",
    "section": "",
    "text": "Esta es la versión preliminar de los simuladores y presentaciones elaborados por los miembros del Laboratorio 25, para apoyar el aprendizaje de los cursos de Aprendizaje y Comportamiento Adaptable que se imparte en la Facultad de Psicología de la UNAM.\nEl proyecto de esta página interactiva web fue financiado por el proyecto PAPIME PE309624.\nArturo Bouzas.",
    "crumbs": [
      "Inicio"
    ]
  },
  {
    "objectID": "DifussionModels/DDM_Ratcliff.html#modelo",
    "href": "DifussionModels/DDM_Ratcliff.html#modelo",
    "title": "Modelo de Difusión de Ratcliff",
    "section": "Modelo",
    "text": "Modelo\n\n\n\nSimulación del Modelo de Difusión de Ratcliff (1976).",
    "crumbs": [
      "Modelos de Difusión",
      "Modelo de Difusión de Ratcliff"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html",
    "href": "GameTheory/Intro_to_game_theory.html",
    "title": "Introducción a Teoría de Juegos",
    "section": "",
    "text": "La teoría de juegos es el estudio de la interacción estratégica, y es el medio para modelar formalmente interacciones entre las personas y prever su comportamiento.\nEl marco metodológico que proporciona Teoría de juegos se utiliza para analizar situaciones en las que los resultados de cada jugador dependen de las decisiones que tomaron en conjunto todos los jugadores. Por lo tanto, la elección de una persona depende de la elección de la otra y viseversa.\nLa teoría de juegos modela todo tipo de interacciones de la vida diaria, por ejemplo, interacciones entre amigos, equipos deportivos, compañías, o candidatos presidenciales. Casi cualquier interacción puede ser modelada como un juego, y podemos predecir el comportamiento utilizando las herramientas que provee esta teoría.\nUn juego de estrategia es cualquier situación en la que: hay dos o más individuos; las acciones de los individuos determinan conjuntamente los resultados; y los individuos conocen las acciones e incentivos propios y de los demás, y utilizan esta información para anticipar estratégicamente qué acciones elegirán.\nExisten cuatro elementos fundamentales en cualquier juego:\nPara acotar las soluciones de un juego se asumen tres supuestos sobre las motivaciones y la capacidad cognitiva de los jugadores:",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#juegos-y-estrategias",
    "href": "GameTheory/Intro_to_game_theory.html#juegos-y-estrategias",
    "title": "Introducción a Teoría de Juegos",
    "section": "Juegos y Estrategias",
    "text": "Juegos y Estrategias\n\nEstrategia Pura\n\nNombre del juego: Dilema del Prisionero\nTipo de ganancias: Suma general\nTipo de jugadas: Simultáneo\nInformación: Completa\n\nUna estrategia pura es un plan que no involucra azar, es decir, se sabe con certeza qué acción tomará el jugador en cada punto de decisión.\nEl Dilema del Prisionero es el juego más famoso de Teoría de Juegos, y fue desarrollado en 1950 con el objetivo de realizar investigaciones para las Fuerzas Armadas de los Estados Unidos. El juego se popularizó por predecir un resultado contraintuitivo, en el dilema del prisionero, cada individuo que persigue racionalmente su interés propio lleva a un resultado perjudicial para todos. Esta paradoja es la razón por la cual este juego ha sido estudiado extensamente y se ha vuelto parte de la cultura popular.\nLos juegos de suma general son aquellos en los que las ganancias de los jugadores no tienen que sumar siempre el mismo valor, lo que permite cooperación o competición ya que el beneficio de un jugador no implica necesariamente la pérdida de otro, esta definición cobrará más sentido cuando se compare con los juegos de suma cero.\nLos juegos simulténaos implican que los jugadores eligen sus estrategias al mismo tiempo, sin conocer las decisiones de los demás, lo que obliga a cada jugador a tomar decisiones basadas en lo que creen que sus rivales elegirán.\nLa información completa quiere decir que todos los jugadores conocen las estrategias, los posibles pagos propios y de los demás, y de las acciones previas de todos los jugadores\n\n\nEstrategia Mixta\n\nNombre del juego: Matching Pennies (Versión Penales)\nTipo de ganancias: Suma cero\nTipo de jugadas: Simultáneo\nInformación: Completa\n\nUna estrategia mixta es cuando los jugadores juegan sus estrategias puras de manera probabilística. Saber lo que hará el otro jugador con certeza puede ser contraproducente en algunas situaciones (por ejemplo, en un juego de póker o un penal en un partido de fútbol), para esto John von Neumann proporcionó una solución. Esta consistía en permitir que los jugadores aleatorizaran, así, incluso si conocen la probabilidad que el otro le asigna a cada estrategia, aún así no saben cuál será su elección final.\nLa probabilidad que un jugador le asigna a su conjunto de estrategias deben sumar 1, no deben ser negativas y se asignan en función de las ganancias de cada jugador. Bajo estas reglas, una estrategia pura puede considerarse como un caso especial de una estrategia mixta, en el que las probabilidades de todas las estrategias son iguales a 0, excepto una, que tiene probabilidad de 1.\nLos juegos de suma cero describen situaciones en las que una persona no puede ganar sin perjudicar a otra.\nEsta interacción podría capturarse igualmente con ganancias que siempre sumen a un cierto número que no sea cero. Por ejemplo, si todos los posibles resultados suman 100 puntos para ambos jugadores, aunque las reparticiones de 100 puntos sean diferentes. Por lo anterior, también se le conoce como juego de suma constante, sin embargo utilizaremos el término juego de suma cero. Lo relevante de este tipo de juegos es que es una interacción en la que los intereses de dos jugadores están estrictamente en conflicto. En los juegos de suma general existe la posibilidad de que amboos jugadores puedan ccooperar para maximizar sus ganancias, en estos juegos no puedes ganar algo sin que el otro lo pierda.\n\n\nEstrategia Grim Trigger\n\nNombre del juego: Confianza\nTipo de ganancias: Suma general\nTipo de jugadas: Secuancial\nInformación: Completa\n\nLa estrategia grim trigger es un plan de acción en el que un jugador empieza eligiendo acciones que impliquen cooperar, y mantendrá su cooperación siempre y cuando su rival también esté cooperando. Sin embargo, si su rival elige una sola acción que lo perjudique no volverá a cooperar en lo que resta del juego.\nEl juego de la confianza, así como el dilema del prisionero, cobra relevancia por representar otro dilema social, es decir, una situación en el que la racionalidad lleva a los participantes del juego a resultados subóptimos. En un ejemplo típico del juego, un jugador es el “inversor” y otro es el “empresario”, el inversor comienza eligiendo si invertir o no en el empresario. Si decide invertir, la cantidad transferida al empresario es multiplica y ahora el empresario debe decidir cuánto devolver al inversor, si es que devuelve algo.\nLos juegos secuenciales son aquellos en los que los jugadores toman decisiones de manera ordenada, uno después del otro, cada jugador puede observar las acciones previas antes de tomar su propia decisión, lo que permite estrategias basadas en la historia del juego.\n\n\nEstrategia Tit for Tat\n\nNombre del juego: Dictador\nTipo de ganancias: Suma cero\nTipo de jugadas: Secuancial\nInformación: Completa\n\nEn 1984 Robert Axelrod llevó a cabo un torneo de estrategias, en el torneo le pidió a economistas, psicólogos, matemáticos y sociólogos que reportaran la estrategia que seguirían en un juego repetido del dilema de prisionero, y reportó aquella estrategia que obtuvo mejores resultados.\nLa estrategia ganadora fue tit-for-tat, dicha estrategia consta en empezar cooperando y copiar lo que sea que el rival haya jugado en la ronda pasada. Esta estrategia es poderosa porque obtiene buenos resultados contra sí misma, debido a que empieza cooperando, no es tan fácil de explotar y es capaz de regresar a cooperar si es que el rival se equivocó.\nEn el juego del dictador a uno de los jugadores le corresponde ser el “dictador”. El dictador recibe una cantidad fija de recursos (por ejemplo, dinero) y debe decidir cómo dividirlos entre él mismo y el otro jugador. El otro jugador solo puede aceptar la partición que le dieron, si es que el dictador le dió algo.",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#soluciones-a-juegos",
    "href": "GameTheory/Intro_to_game_theory.html#soluciones-a-juegos",
    "title": "Introducción a Teoría de Juegos",
    "section": "Soluciones a Juegos",
    "text": "Soluciones a Juegos\n\nSolución de un juego por Dominancia\n\nNombre del juego: Dilema del Prisionero y Juego del esfuerzo\nTipo de ganancias: Suma general\nTipo de jugadas: Simultáneo\nInformación: Completa\n\nUna estrategia es estrictamente dominante cuando todos los resultados de esa estrategia son estrictamente más favoralbles que los resultados de cualquier otra estrategia. El proceso que hacen los jugadores de eliminar estrategias estrictamente dominadas se le llama eliminación iterativa de estrategias dominadas, y este proceso te puede llevar a la solución de un juego.\n\n\nSolución de un juego por Maximin y Minimax\n\nNombre del juego: Candidatos políticos\nTipo de ganancias: Suma cero\nTipo de jugadas: Simultáneo\nInformación: Completa\n\nEn muchos juegos los jugadores no tendrán estrategias dominadas, por lo que el método minimax/maximin es una manera de llegar a la solución de un juego asumiendo que los jugadores utilizan una regla de dedo, es decir, un atajo cognitivo o “regla de oro”. La regla de dedo es simple, la idea básica es que cada jugador asume que, para cada estrategia que podría jugar, terminará con la menor ganancia posible de esa estrategia. Así que dentro de los peores escenarios el jugador elige el que ofrezca la mayor ganancia, en otras palabras, el jugador actúa para maximizar su ganancia mínima (maximin), mientras que asume que el rival actúa para minimizar su ganancia máxima (minimax).\nLos juegos de suma cero no suelen tener solución por dominancia y esta solución fue creada particularmente para juagos de suma cero, ya que cuando las ganancias de un jugador no son las pérdidas del otro, ya no es razonable asumir que el rival siempre actuará para minimizar ganancias del jugador.\n\n\nSolución de un juego por Equilibrio de Nash\n\nTipo de ganancias: Suma general\nTipo de jugadas: Simultáneo\nInformación: Completa\n\nA comparación de las anteriores dos soluciones, este es el enfoque más amplio, ya que funciona para cualquier juego y su solución coincide con las soluciones a las que se llega a dominancia o minimax/maximin, por esta razón es que esta es la solución más usada y reconocida en Teoría de Juegos.\nDeterminar la estrategia que será seleccionada por cada jugador de acuerdo al Equilibrio de Nash consta de los siguientes pasos: - Primero seleccionamos la ganancia máxima de un jugador para cada estrategia del rival, las estrategias asociadas a esas ganancias serán sus mejores respuestas para cada estrategia del rival. - Después hacemos lo mismo con las ganancias y estrategias del rival. - Ya que conocemos la mejor respuesta de cada participante para cada una de las estrategias del otro, ahora buscamos un resultado donde las ganancias de ambos jugadores fueron seleccionadas, y las estrategias asociadas a esa superposición es el conjunto de estrategias que le permite a ambos jugadores dar su mejor respuesta mutuamente. - Si esta superposición no existe, entonces el Equilibrio de Nash es que ambos jugadores seleccionen de sus estrategias con cierta probabilidad, es decir, es un conjunto de estrategias mixtas.\nPor lo anterior, un Equilibrio de Nash indica que cada jugador prevee perfectamente lo que hará su rival y selecciona la mejor respuesta a esas creencias, y esas creencias siempre son correctas. Debido a que los jugadores tienen creencias correctas y están dando su mejor respuesta, ninguno de los jugadores desea desviarse unilateralmente de su propia estrategia dada la estrategia del otro jugador. Lo que hace el otro jugador está fuera del control del jugador, pero un jugador racional siempre lo anticipará y responderá de la mejor manera.",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#juegos-con-n-estrategias-y-n-jugadores",
    "href": "GameTheory/Intro_to_game_theory.html#juegos-con-n-estrategias-y-n-jugadores",
    "title": "Introducción a Teoría de Juegos",
    "section": "Juegos con n estrategias y n jugadores",
    "text": "Juegos con n estrategias y n jugadores\n\nNombre del juego: Bienes comunes\nTipo de ganancias: Suma general\nTipo de jugadas: Secuencial\nInformación: Completa\n\nHasta este punto, hemos considerado únicamente juegos con dos jugadores y un número finito de estrategias (solo dos o tres), estos juegos pueden representarse con matrices de ganancias como ya hemos ilustrado. Sin embargo, hay juegos donde los jugadopres pueden tener un número infinito de estrategias, o un número arbitrariamente grande de jugadores, o ambos.\nLos juegos con \\(n\\) estrategias y \\(n\\) jugadores son juegos que no pueden ser representados con matrices, así que son representados con funciones. Además, las soluciones de estos juegos son más complejas, por ejemplo; si las estrategias son continuas, la solución implica usar cálculo para derivar la estrategia que maximiza las ganancias de un jugador; si hay muchos jugadores con incentivos idénticos, podemos buscar un resultado simétrico en el que todos elijan hacer lo mismo.\nEl juego de bienes comunes consta en que existe un recurso para un grupo de jugadores (por ejemplo, un recurso natural), y cada jugador puede elegir entre cooperar, usando el recurso de manera sostenible, o actuar de manera egoísta y explotar, extrayendo más de lo justo. Este tipo de juego ilustra el dilema de “la tragedia de los comunes”, donde los intereses individuales llevan a un resultado subóptimo para todo un grupo, esto si no existen incentivos que motiven a los jugadores a cooperar.",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#teoría-de-juegos-psicológicos",
    "href": "GameTheory/Intro_to_game_theory.html#teoría-de-juegos-psicológicos",
    "title": "Introducción a Teoría de Juegos",
    "section": "Teoría de juegos Psicológicos",
    "text": "Teoría de juegos Psicológicos\n\nEstrategia del rival: Modelo de Aversión a la Inequidad\nNombre del juego: Ultimatum\nTipo de ganancias: Suma cero\nTipo de jugadas: Secuencial\nInformación: Completa\n\nEn algunas situaciones de Teoría de Juegos las personas se desvían de las soluciones derivadas lógica o matemáticamente. Esto se debe a que Teoría de Juegos asume que a los jugadores solo les importa las ganancias que obtendrán al final del juego. Sin embargo, las elecciones de los jugadores demuestran que, además del resultado final, sus preferencias también incluyen motivaciones psicológicas, como altruismo, reciprocidad, equidad, reputación, emociones como culpa y egoísmo, etc. Por lo anterior, se crean modelos que incluyan dichas motivaciones, y así dar cuenta a las desviaciones conductuales observadas en las personas.\nEl juego del ultimátum es uno de los juegos más conocidos de teoría de juegos y es un caso especial del juego de negociación. Es aquel en el que solo hay una ronda para que el jugador oferte una partición del presupuesto y otra ronda para que el rival acepte o rechace la partición. La solución de este juego indica que el jugador debe ofrecer la menor cantidad posible del presupuesto, y el rival debe aceptar cualquier oferta que sea mayor a cero, o mantenerse indiferente entre aceptar o rechazar si le ofrecen cero. Sin embargo, la relevancia de este juego radica en que los jugadores se desvían de la solución planteada, y deben pensar en cuánto ofrecer para no ser rechazados y recibir una recompensa.\nEl modelo de Aversión a la Inequidad de Fher y Schmidt considera que hay personas que les importa sus ganancias y las ganancias de otros jugadores, por esto una recompensa es percibida diferente dependiendo de lo que obtuvo cada quien. El modelo es conocido por explicar conducta justa, competitiva o cooperativa. El modelo solo contiene dos parámetros que ponderan la utilidad total de las ganancias de un jugador, donde (\\(alpha, \\alpha\\)) puede ser interpretado estratégicamente como aversión a la desventaja y (\\(beta, \\beta\\)) como aversión a la ventaja, o \\(\\alpha\\) se suele interpretar como egoísmo y \\(\\beta\\) como culpa.",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#aprendizaje-en-teoría-de-juegos-reinforcement-learning",
    "href": "GameTheory/Intro_to_game_theory.html#aprendizaje-en-teoría-de-juegos-reinforcement-learning",
    "title": "Introducción a Teoría de Juegos",
    "section": "Aprendizaje en Teoría de Juegos: Reinforcement Learning",
    "text": "Aprendizaje en Teoría de Juegos: Reinforcement Learning\n\nJuego: Negociación\nAgente: Reinforcement Learning\n\nEl juego de negociación representa cualquier situación en la que dos jugadores deben acordar cómo repartirse algo que aporta utilidad a todas las partes involucradas (e.g. un presupuesto total, un pastel para todos, etc.), y si no son capaces de acordar una partición ninguna parte recibirá utilidad. La importancia del juego radica en qué ofertas o contraofertas se deben hacer para no irse con las manos vacias de esa negociación pero sacando el mayor provecho posible.\nEn lugar de requerir que los jugadores siempre respondan perfectamente a creencias correctas, el modelo Reinforcement Learning (RL) asume que los jugadores le asignan más probabilidad de elección a estrategias que han sido particularmente rentables en el pasado.\nSimplificando, Reinforcement Learning es un modelo de aprendizaje que le asigna la misma probabilid de elección a cada una de sus estrategias puras, pero conforme pasan los ensayos (simulaciones) aprende que algunas estrategias son más rentables que otras y deberían jugarse con más frecuencia. El agente aumenta su probabilidad de jugar una estrategia cada vez que la elige y le va bien. Cada vez que le toca jugar, el agente RL juega cada estrategia pura con una probabilidad relativa al valor que le ha asignado a cada estrategia.",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#aprendizaje-en-juegos-functional-experience-weighted-atraction",
    "href": "GameTheory/Intro_to_game_theory.html#aprendizaje-en-juegos-functional-experience-weighted-atraction",
    "title": "Introducción a Teoría de Juegos",
    "section": "Aprendizaje en Juegos: Functional Experience Weighted Atraction",
    "text": "Aprendizaje en Juegos: Functional Experience Weighted Atraction\n\nJuego: Negociación\nAgente: Functional Experience Weighted Atraction\n\nFunctional Experience Weighted Attraction (FEWA) es otro modelo de aprendizaje derivado de Experience Weighted Attraction (EWA), sin embargo, esta iteración del modelo contiene parámetros que se autoajustan, y fue creado en respuesta a las críticas por la cantidad de parámetros que hay que estimar en EWA. El modelo inicial EWA, del que está basado FEWA, es un híbrido que combina Aprendizaje por Refuerzo (RL) y Aprendizaje por Creencias (Belief Learning ; BL, por sus siglas en inglés). En otras palabras, FEWA toma en consideración su desempeño total al hacer un conteo de las ganancias que obtuvo por acción (RL), y también toma en cuenta el desempeño promedio del rival al hacer un conteo de las acciones que este eligió (BL).",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#widget-interactivo",
    "href": "GameTheory/Intro_to_game_theory.html#widget-interactivo",
    "title": "Introducción a Teoría de Juegos",
    "section": "Widget interactivo",
    "text": "Widget interactivo\nPuedes acceder a la versión interactiva de esta introducción a Teoría de Juegos, donde podrás jugar los distintos juegos:\n\nGoogle Colab: Introducción a Teoría de Juegos",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "GameTheory/Intro_to_game_theory.html#referencias",
    "href": "GameTheory/Intro_to_game_theory.html#referencias",
    "title": "Introducción a Teoría de Juegos",
    "section": "Referencias",
    "text": "Referencias\n\n\nCamerer, C., & Ho, T.-H. (1999). Experienced-Weighted Attraction Learning in Normal Form Games. Econometrica, 67(4), 827-874. http://www.jstor.org/stable/2999459\n\n\nCarpenter, J., & Robbett, A. (2022). Game Theory and Behavior. MIT Press.\n\n\nFehr, E., & Schmidt, K. M. (1999). A Theory of Fairness, Competition, and Cooperation. The Quarterly Journal of Economics, 114(3), 817-868. https://doi.org/10.1162/003355399556151\n\n\nNash, J. F. (1950). Equilibrium points in n -person games. Proceedings of the National Academy of Sciences, 36(1), 48-49. https://doi.org/10.1073/pnas.36.1.48\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement Learning, second edition. MIT Press.",
    "crumbs": [
      "Teoría de Juegos",
      "Introducción a Teoría de Juegos"
    ]
  },
  {
    "objectID": "ANN/IntroANN.html",
    "href": "ANN/IntroANN.html",
    "title": "Introducción a Redes Neuronales",
    "section": "",
    "text": "Las redes neuronales artificiales son un modelo computacional inspirado en el cerebro humano. Están compuestas por nodos llamados neuronas que están conectados entre sí. Cada conexión entre neuronas tiene un peso asociado que se ajusta durante el entrenamiento del modelo. Estos pesos son los parámetros que se ajustan para que el modelo pueda realizar predicciones, es decir, son la memoria del modelo y representan la importancia de cada conexión.\nLas redes neuronales artificiales se dividen en capas, cada capa está compuesta por un conjunto de neuronas. La primera capa se llama capa de entrada, la última capa se llama capa de salida y las capas intermedias se llaman capas ocultas. La capa de entrada recibe los datos de entrada, la capa de salida produce la predicción y las capas ocultas procesan la información.",
    "crumbs": [
      "Modelos de Redes Neuronales",
      "Introducción a Redes Neuronales"
    ]
  },
  {
    "objectID": "ANN/IntroANN.html#comportamiento-de-una-neurona",
    "href": "ANN/IntroANN.html#comportamiento-de-una-neurona",
    "title": "Introducción a Redes Neuronales",
    "section": "Comportamiento de una Neurona",
    "text": "Comportamiento de una Neurona\nCada neurona recibe una serie de entradas, las multiplica por los pesos asociados a cada conexión y aplica una función de activación. La función de activación es una función no lineal que se encarga de introducir no linealidades en el modelo.\nMatemáticamente, el comportamiento de una neurona se puede expresar de la siguiente forma:\n\\[y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)\\]\nDonde \\(x_i\\) son las entradas, \\(w_i\\) son los pesos asociados a cada conexión, \\(b\\) es el sesgo y \\(f\\) es la función de activación. Si usaramos una función de activación lineal, la red neuronal sería equivalente a un modelo de regresión lineal.\nEntonces podemos usar una red neuronal para regresión lineal:\n\\[\\begin{align*}\ny &= f(\\sum_{i=1}^{n} x_i \\cdot w_i + b) \\hspace{1cm} \\text{Donde } f(x) = x \\\\\ny &= \\sum_{i=1}^{n} x_i \\cdot w_i + b\n\\end{align*}\\]\nExisten diversas funciones de activación, algunas de las más comunes son:\n\nFunción Sigmoide. \\[f(x) = \\frac{1}{1 + e^{-x}}\\]\nFunción ReLU. \\[f(x) = \\max(0, x)\\]\nFunción Tangente Hiperbólica. \\[f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\]\nFunción Softmax. \\[f(x) = \\frac{e^x}{\\sum_{i=1}^{n} e^{x_i}}\\]\nFunción de Identidad. \\[f(x) = x\\]\n\nCada función de activación tiene sus propias características y se utiliza en diferentes contextos. Por ejemplo, la función sigmoide se utiliza en la capa de salida de una red neuronal para clasificación binaria, la función ReLU se utiliza en las capas ocultas y la función softmax se utiliza en la capa de salida para clasificación multiclase.\nVisualicemos el comportamiento de algunas funciones de activación:\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 4), sharex=True)\n\nx = np.linspace(-5, 5, 100)\n\n# Funciones de activación\nsigmoid = 1 / (1 + np.exp(-x))\nrelu = np.maximum(0, x)\ntanh = np.tanh(x)\nsoftmax = np.exp(x) / np.sum(np.exp(x))\n\n# Gráficas\nax[0, 0].plot(x, sigmoid)\nax[0, 0].set_title(\"Función Sigmoide\")\n\nax[0, 1].plot(x, relu)\nax[0, 1].set_title(\"Función ReLU\")\n\nax[1, 0].plot(x, tanh)\nax[1, 0].set_title(\"Función Tangente Hiperbólica\")\n\nax[1, 1].plot(x, softmax)\nax[1, 1].set_title(\"Función Softmax\")\n\nfor i in range(2):\n    for j in range(2):\n        ax[i, j].grid( linestyle='--', linewidth=0.5, alpha=0.5, color='grey')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHagamos lo que hace una neurona con una función de activación sigmoide.\n\n\nCódigo\n# Función de activación sigmoide\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Datos de entrada\nX = np.random.randn(10, 1)\n\n# Pesos y sesgo\nnp.random.seed(1014)\nweights = np.random.randn(1, 10)\nbias = np.random.randn(1)\n\n# Salida de la neurona\ny = sigmoid(np.dot(X.T, weights.T) + bias)\n\nprint(y)\n\n\n[[0.00261715]]\n\n\nEstamos realizando la siguiente operación:\n\\[y = f(\\sum_{i=1}^{n} x_i \\cdot w_i + b)\\]\nDonde \\(f(x) = \\frac{1}{1 + e^{-x}}\\) es la función sigmoide. En este caso, estamos utilizando una neurona con 10 entradas y una salida.\nLo que busca simular o modelar el comportamiento de una neurona biológica. La neurona biológica recibe señales eléctricas de otras neuronas a través de las dendritas, las procesa en el cuerpo celular y envía una señal eléctrica a través del axón. La señal eléctrica se transmite a través de las sinapsis, que son las conexiones entre las neuronas.\nAhora veamos cómo se comporta una red neuronal con una capa oculta y una capa de salida. Para esto, vamos a implementar una red neuronal para regresión lineal con pesos y sesgos aleatorios.\n\n\nCódigo\nimport numpy as np\n\nclass NeuralNetwork():\n    def __init__(self, input_size, hidden_size, output_size, seed=1014):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Pesos y sesgos\n        np.random.seed(seed)\n        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n        self.bias_input_hidden = np.random.randn(hidden_size)\n        \n        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n        self.bias_hidden_output = np.random.randn(output_size)\n        \n    def forward(self, X, activation):\n        # Capa oculta\n        hidden = np.dot(X, self.weights_input_hidden) + self.bias_input_hidden\n        hidden = activation(hidden)\n        \n        # Capa de salida\n        output = np.dot(hidden, self.weights_hidden_output) + self.bias_hidden_output\n        \n        return output\n\n\nUsemos la red neuronal para predecir un conjunto de datos y con una función de activación identidad.\n\n\nCódigo\n# Datos de entrada\nX = np.random.randn(10, 1)\n\n# Parámetros de la red neuronal\ninput_size = 1\nhidden_size = 10\noutput_size = 1\n\n# Red Neuronal\nnn = NeuralNetwork(input_size, hidden_size, output_size)\n\n# Función de activación identidad\ndef identity(x):\n    return x\n\n# Predicciones\ny_pred = nn.forward(X, identity)\n\nprint(y_pred)\n\n\n[[-1.81046445]\n [-1.98312314]\n [-1.86441218]\n [-2.03739283]\n [-1.77027647]\n [-1.92937854]\n [-1.87128124]\n [-1.94289359]\n [-2.20644573]\n [-1.93053922]]\n\n\nEn este caso tenemos una red neuronal con una capa oculta de 10 neuronas y una capa de salida de 1 neurona. La función de activación de la capa oculta es la función identidad y la función de activación de la capa de salida también es la función identidad.\nLos pesos de la red son:\n\n\nCódigo\nprint(f\"Pesos capa oculta: {nn.weights_input_hidden}\\n\")\nprint(f\"Sesgos capa oculta: {nn.bias_input_hidden}\\n\")\nprint(f\"Pesos capa de salida: {nn.weights_hidden_output.T}\\n\")\nprint(f\"Sesgos capa de salida: {nn.bias_hidden_output}\\n\")\n\n\nPesos capa oculta: [[ 0.75943278 -1.00725987 -0.64499024 -0.26674068  0.29125552  0.14820587\n   0.6382988   0.46738854  0.53122954  1.1840206 ]]\n\nSesgos capa oculta: [-1.25173295 -1.30489407  0.20194032 -0.83407915  0.67556507 -1.65562438\n -0.26710189 -0.77413114 -0.14915279  2.15093091]\n\nPesos capa de salida: [[-0.25697236  0.50673502 -1.31687341  1.71747235 -0.12242967 -0.06419002\n   0.47479916 -0.01225672  1.10530347 -0.54019387]]\n\nSesgos capa de salida: [1.4985733]\n\n\n\nPodemos dibujar la red neuronal con los pesos y sesgos asociados a cada conexión.\n\n\nCódigo\nimport itertools\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n# Colores para las capas\nsubset_colors = ['blue', 'red', 'green']\n\ndef multilayered_graph(input_size, hidden_size, output_size, weights_input_hidden, weights_hidden_output):\n    # Crear los rangos para las capas\n    subset_sizes = [input_size, hidden_size, output_size]\n    extents = nx.utils.pairwise(itertools.accumulate((0,) + tuple(subset_sizes)))\n    layers = [range(start, end) for start, end in extents]\n    \n    # Crear el gráfico\n    G = nx.Graph()\n    for i, layer in enumerate(layers):\n        G.add_nodes_from(layer, layer=i)\n        \n    # Añadir los bordes con pesos para capa de entrada a oculta\n    for i, j in itertools.product(layers[0], layers[1]):\n        G.add_edge(i, j, weight=round(weights_input_hidden[i, j - layers[1][0]], 3))\n        \n    # Añadir los bordes con pesos para capa oculta a salida\n    for i, j in itertools.product(layers[1], layers[2]):\n        G.add_edge(i, j, weight=round(weights_hidden_output[i - layers[1][0], j - layers[2][0]], 3))\n    \n    return G\n\n# Crear el gráfico con los pesos\nG = multilayered_graph(input_size, hidden_size, output_size, nn.weights_input_hidden, nn.weights_hidden_output)\n\n# Colores para los nodos según su capa\ncolor = [subset_colors[data[\"layer\"]] for node, data in G.nodes(data=True)]\n\n# Posición de los nodos\npos = nx.multipartite_layout(G, subset_key=\"layer\")\n\n# Dibujar el gráfico\nplt.figure(figsize=(10, 6))\nnx.draw(G, pos, with_labels=False, node_color=color, node_size=1500, font_size=10, font_weight='bold')\n\n# Dibujar los bordes con los pesos\nedge_labels = nx.get_edge_attributes(G, 'weight')\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCada una de las neuronas realiza la operación que hemos visto anteriormente. Sin embargo, aquí solo hemos decidido los pesos y sesgos de la red neuronal de forma aleatoria. En la práctica, estos pesos y sesgos se ajustan durante el entrenamiento de la red neuronal, es decir, la red neuronal aprende a partir de los datos.",
    "crumbs": [
      "Modelos de Redes Neuronales",
      "Introducción a Redes Neuronales"
    ]
  },
  {
    "objectID": "ANN/IntroANN.html#entrenamiento-de-una-red-neuronal",
    "href": "ANN/IntroANN.html#entrenamiento-de-una-red-neuronal",
    "title": "Introducción a Redes Neuronales",
    "section": "Entrenamiento de una Red Neuronal",
    "text": "Entrenamiento de una Red Neuronal\nEl entrenamiento de una red neuronal consiste en ajustar los pesos y sesgos de la red para minimizar una función de pérdida. La función de pérdida mide la diferencia entre las predicciones del modelo y los valores reales. Durante el entrenamiento, los pesos y sesgos se ajustan iterativamente utilizando un algoritmo de optimización.\nExisten diversos algoritmos de optimización, algunos de los más comunes son:\n\nDescenso del Gradiente : Actualiza los pesos en la dirección opuesta al gradiente de la función de pérdida.\nAdam: Utiliza una combinación de descenso del gradiente y adaptación de la tasa de aprendizaje.\nRMSprop: Se adapta a la tasa de aprendizaje para cada parámetro.\nAdagrad: Ajusta la tasa de aprendizaje para cada parámetro en función de la magnitud de los gradientes.\n\nEl algoritmo tipico es el descenso del gradiente. La idea es ajustar los pesos y sesgos de la red neuronal en la dirección opuesta al gradiente de la función de pérdida. El gradiente de la función de pérdida se calcula utilizando la regla de la cadena y el algoritmo de retropropagación. Pero en la práctica, se utiliza una variante del descenso del gradiente llamada descenso del gradiente estocástico o el algoritmo Adam.\nAlgunas funciones de pérdida comunes son:\n\nError Cuadrático Medio. \\[L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nEntropía Cruzada. \\[L(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\\]\nError Absoluto Medio. \\[L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\]\n\nEl algoritmo de entrenamiento de una red neuronal se puede resumir en los siguientes pasos:\n\nInicializar los pesos y sesgos de la red neuronal.\nCalcular la salida de la red neuronal.\nCalcular la función de pérdida.\nCalcular el gradiente de la función de pérdida.\nActualizar los pesos y sesgos utilizando un algoritmo de optimización.\nRepetir los pasos 2-5 hasta que se alcance un número de iteraciones o se cumpla un criterio de parada.\n\n\nAprendizaje de pesos y sesgos en una neurona\nVamos a ver cómo se actualizan los pesos y sesgos de una neurona durante el entrenamiento. Para esto, vamos a implementar una neurona con una función de activación sigmoide y vamos a entrenar la neurona para realizar una regresión lineal y usar la función de pérdida de error cuadrático medio.\nPara obtener el gradiente de la función de pérdida, vamos a utilizar la regla de la cadena y el algoritmo de retropropagación. La regla de la cadena se utiliza para calcular el gradiente de una función compuesta y el algoritmo de retropropagación se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal.\n\nRegla de la Cadena para el Gradiente\nLa regla de la cadena se utiliza para calcular el gradiente de una función compuesta. Si tenemos una función \\(f(g(x))\\), el gradiente de \\(f\\) con respecto a \\(x\\) se puede calcular como:\n\\[\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)} \\cdot \\frac{\\partial g(x)}{\\partial x}\\]\nEn nuestro caso tenemos una función de pérdida \\(L(y, \\hat{y})\\) y una función de activación \\(f(x)\\). Entonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n\\[\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} = \\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}\\]\nDonde \\(z = \\sum_{i=1}^{n} x_i \\cdot w_i + b\\) es la entrada de la neurona y \\(\\hat{y} = f(z)\\) es la salida de la neurona.\nHagamos la primera parte de la regla de la cadena, es decir, el gradiente de la función de pérdida con respecto a la salida de la neurona.\n\\[\\begin{align*}\n\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}} &= \\frac{\\partial}{\\partial \\hat{y}} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\hat{y}} (y_i - \\hat{y}_i)^2 \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\hat{y}_i) \\\\\n&= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i)\n\\end{align*}\\]\nLa segunda parte de la regla de la cadena es el gradiente de la salida de la neurona con respecto a la entrada de la neurona.\n\\[\\begin{align*}\n\\frac{\\partial \\hat{y}}{\\partial z} &= \\frac{\\partial}{\\partial z} \\left( \\frac{1}{1 + e^{-z}} \\right) \\\\\n&= \\frac{e^{-z}}{(1 + e^{-z})^2} \\\\\n&= \\frac{1}{1 + e^{-z}} \\cdot \\left(1 - \\frac{1}{1 + e^{-z}} \\right) \\\\\n&= \\hat{y} \\cdot (1 - \\hat{y})\n\\end{align*}\\]\nLa tercera parte de la regla de la cadena es el gradiente de la entrada de la neurona con respecto a los pesos.\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n&= x_i\n\\end{align*}\\]\nAhora para el sesgo.\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left( \\sum_{i=1}^{n} x_i \\cdot w_i + b \\right) \\\\\n&= 1\n\\end{align*}\\]\nEntonces, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal se puede calcular como:\n\\[\\begin{align*}\n\\frac{\\partial L(y, \\hat{y})}{\\partial w_i} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i) \\cdot x_i \\\\\n\\frac{\\partial L(y, \\hat{y})}{\\partial b} &= \\frac{-2}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i) \\cdot \\hat{y}_i \\cdot (1 - \\hat{y}_i)\n\\end{align*}\\]\n\nActualización de los pesos y sesgos\nPara actualizar los pesos y sesgos de la red neuronal, utilizamos el algoritmo de descenso del gradiente. La actualización de los pesos y sesgos se realiza de la siguiente forma:\n\\[\\begin{align*}\nw_i &:= w_i - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial w_i} \\\\\nb &:= b - \\alpha \\cdot \\frac{\\partial L(y, \\hat{y})}{\\partial b}\n\\end{align*}\\]\nDonde \\(\\alpha\\) es la tasa de aprendizaje, que es un hiperparámetro del modelo. La tasa de aprendizaje controla la magnitud de la actualización de los pesos y sesgos. Si la tasa de aprendizaje es muy pequeña, el modelo puede tardar mucho tiempo en converger. Si la tasa de aprendizaje es muy grande, el modelo puede divergir.\n\n\n\nRetropropagación del Gradiente.\nLa retropropagación es un algoritmo que se utiliza para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal, usamos la regla de la cadena para calcular el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y lo propagamos hacia atrás a través de la red neuronal.\nPara cada capa de la red neuronal, calculamos el gradiente de la función de pérdida con respecto a los pesos y sesgos de la capa utilizando la regla de la cadena y el gradiente de la capa anterior. Luego, actualizamos los pesos y sesgos de la capa. Este proceso se repite para todas las capas de la red neuronal. Un hermoso gif creado por Michael Pyrcz muestra cómo funciona la retropropagación.\n\n\n\nRetropropagación del Gradiente\n\n\n\n\n\nImplementación de Backpropagation\nHagamos una red neuronal con 1 neurona en la capa oculta y 5 neuronas en la capa de salida, usaremos la función de activación sigmoide para la capa oculta y la función de activación identidad para la capa de salida. Vamos a entrenar la red neuronal para realizar una regresión lineal y usaremos la función de pérdida de error cuadrático medio.\n\nDatos de Entrada\nSimulemos datos de entrada y salida para entrenar la red neuronal.\n\n\nCódigo\nimport numpy as np\nimport pandas as pd\n\n# Datos de entrada\nX = np.random.normal(0, 5, (40, 1))\n\n# Datos de salida\ny = 2 * X + 3 + np.random.normal(0, 1, (40, 1))\n\ndf = pd.DataFrame(np.concatenate([X, y], axis=1), columns=[\"X\", \"y\"])\ndf.head()\n\n\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n1.705603\n5.973504\n\n\n1\n-6.149390\n-8.670173\n\n\n2\n5.814690\n13.924261\n\n\n3\n2.384975\n6.429876\n\n\n4\n0.502252\n2.931682\n\n\n\n\n\n\n\n\n\n\nFunciones auxiliares\nVamos a implementar algunas funciones auxiliares para la red neuronal.\n\n\nCódigo\n# Función de activación sigmoide\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Función de pérdida de error cuadrático medio\ndef mse(y, y_pred):\n    return np.mean((y - y_pred) ** 2)\n\ndef weight_derivative_hidden(X, y, y_pred):\n    \"\"\"\n    Derivada de los pesos de la capa oculta\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred) * X)\n\ndef bias_derivative_hidden(y, y_pred):\n    \"\"\"\n    Derivada del sesgo de la capa oculta\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * y_pred * (1 - y_pred))\n\ndef weight_derivative_output(hidden, y, y_pred):\n    \"\"\"\n    Derivada de los pesos de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n    \"\"\"\n    return -2 * np.mean((y - y_pred) * X)\n\ndef bias_derivative_output(y, y_pred):\n    \"\"\"\n    Derivada del sesgo de la capa de salida para la función de pérdida de error cuadrático medio y función de activación identidad\n    \"\"\"\n    return -2 * np.mean(y - y_pred)\n\n# Inicialización de los pesos y sesgos\ndef initialize_weights(input_size, hidden_size, output_size, seed=1014):\n    np.random.seed(seed)\n    weights_input_hidden = np.random.randn(input_size, hidden_size)\n    bias_input_hidden = np.random.randn(hidden_size)\n    \n    weights_hidden_output = np.random.randn(hidden_size, output_size)\n    bias_hidden_output = np.random.randn(output_size)\n    \n    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output\n\n\n\n\nEntrenamiento de la Red Neuronal\nVamos a entrenar la red neuronal utilizando el algoritmo de retropropagación. Durante el entrenamiento, vamos a calcular la función de pérdida, el gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal y vamos a actualizar los pesos y sesgos utilizando el algoritmo de descenso del gradiente.\n\n\nCódigo\ndef learning( X, y, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, learning_rate=0.01):\n    # Capa oculta\n    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n    hidden = sigmoid(hidden)\n    \n    # Capa de salida\n    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n    \n    # Función de pérdida\n    loss = mse(y, output)\n    \n    # Gradiente de la función de pérdida con respecto a los pesos y sesgos de la red neuronal\n    weight_der_hidden = weight_derivative_hidden(X, y, output)\n    bias_der_hidden = bias_derivative_hidden(y, output)\n    \n    weight_der_output = weight_derivative_output(hidden, y, output)\n    bias_der_output = bias_derivative_output(y, output)\n    \n    # Actualización de los pesos y sesgos\n    weights_input_hidden -= learning_rate * weight_der_hidden\n    bias_input_hidden -= learning_rate * bias_der_hidden\n    \n    weights_hidden_output -= learning_rate * weight_der_output\n    bias_hidden_output -= learning_rate * bias_der_output\n    \n    return weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss\n\n# Creamos un bucle para entrenar la red neuronal\n\ninput_size = 1\nhidden_size = 5\noutput_size = 1\n\nweights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n\nlearning_rate = 0.0001\nepochs = 1500\n\nX_normalized = (X - X.mean()) / X.std()\nY_normalized = (y - y.mean()) / y.std()\n\nfor epoch in range(epochs):\n    weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, loss = learning(\n    X_normalized, Y_normalized, \n    weights_input_hidden, \n    bias_input_hidden, \n    weights_hidden_output, \n    bias_hidden_output, \n    learning_rate)\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: Loss {loss}\")\n    elif epoch == epochs - 1:\n        print(f\"Epoch {epoch}: Loss {loss}\")\n\n\nEpoch 0: Loss 9.962358063298385\nEpoch 100: Loss 6.389020521754107\nEpoch 200: Loss 4.640115312713357\nEpoch 300: Loss 3.593583112789311\nEpoch 400: Loss 2.893864014205564\nEpoch 500: Loss 2.404086938164928\nEpoch 600: Loss 2.0676644655384124\nEpoch 700: Loss 1.8628933379595423\nEpoch 800: Loss 1.7838514284596045\nEpoch 900: Loss 1.830931639664496\nEpoch 1000: Loss 2.0075290899566314\nEpoch 1100: Loss 2.323764763823051\nEpoch 1200: Loss 2.810902870761798\nEpoch 1300: Loss 3.5596363395467696\nEpoch 1400: Loss 4.844869738072707\nEpoch 1499: Loss 7.69238941777333\n\n\n\n\n\nPredicciones de la Red Neuronal\nUna ves que hemos entrenado la red neuronal, podemos hacer predicciones con la red neuronal que es el objetivo de crear un modelo de aprendizaje automático.\n\n\nCódigo\ndef predict(X, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output):\n    hidden = np.dot(X, weights_input_hidden) + bias_input_hidden\n    hidden = sigmoid(hidden)\n    \n    output = np.dot(hidden, weights_hidden_output) + bias_hidden_output\n    \n    return output\n\ny_pred = predict(X_normalized, weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output)\n\ndf_normalized = pd.DataFrame(np.concatenate([X_normalized, Y_normalized, y_pred], axis=1), columns=[\"X\", \"y\", \"y_pred\"])\n\ndf_normalized.head()\n\n\n\n\n\n\n\n\n\n\nX\ny\ny_pred\n\n\n\n\n0\n0.223278\n0.172289\n1.025840\n\n\n1\n-1.426558\n-1.375204\n3.560325\n\n\n2\n1.086337\n1.012498\n0.068435\n\n\n3\n0.365971\n0.220517\n0.829005\n\n\n4\n-0.029469\n-0.149160\n1.409019\n\n\n\n\n\n\n\n\nA nivel profesional, se utilizan librerías como TensorFlow, PyTorch o Keras para implementar redes neuronales. Estas librerías proporcionan una interfaz de alto nivel para construir y entrenar redes neuronales de forma eficiente. Sin embargo, es importante entender cómo funcionan las redes neuronales a nivel de bajo nivel para poder depurar y optimizar los modelos.",
    "crumbs": [
      "Modelos de Redes Neuronales",
      "Introducción a Redes Neuronales"
    ]
  },
  {
    "objectID": "DifussionModels/DFT_Busemeyer.html",
    "href": "DifussionModels/DFT_Busemeyer.html",
    "title": "Modelo Decision Field Theory de Busemeyer",
    "section": "",
    "text": "El modelo Decision Field Theory de Busemeyer (1992, 1993) surge como a una alternativa a los modelos de decisión clásicos en el estudio de toma de decisión bajo riesgo e incertidumbre. Su objetivo es modelar el proceso de deliberación que lleva a la toma de decisiones, en lugar de simplemente describir el resultado de la decisión. Para ello se busca modelar dos fenómenos ampliamente documentados en la literatura de toma de decisiones.\nEl primer punto se refiere a que la transitividad entre las preferencias no siempre se cumple, es decir, si una persona prefiere A a B y B a C, no necesariamente preferirá A a C. El segundo punto da cuenta que la probabilidad de elección entre varía conforme pasa el tiempo, dado que se va acumulando evidencia a favor de una opción causando que con menor cantidad de tiempo se elija una alternativa que podría no ser la preferida con mayor tiempo de deliberación.",
    "crumbs": [
      "Modelos de Difusión",
      "Modelo Decision Field Theory de Busemeyer"
    ]
  },
  {
    "objectID": "DifussionModels/DFT_Busemeyer.html#construcción-del-modelo",
    "href": "DifussionModels/DFT_Busemeyer.html#construcción-del-modelo",
    "title": "Modelo Decision Field Theory de Busemeyer",
    "section": "Construcción del Modelo",
    "text": "Construcción del Modelo\nPara constuir el modelo Busemyer en su artículo Decision field theory: A dynamic-cognitive approach to decision making in an uncertain environment (1993) da una serie de bloques que se añaden para crear el modelo basandose en modelos previos de toma de decisiones. En esta sección se resumira la construcción del modelo.\n\n\n\n\n\n\nFigura 1: Construcción del modelo Decision Field Theory. Obtenida de Busemeyer y Townsend (1993).\n\n\n\n\nModelos de Utilidad Esperada Subjetiva\n\nUtilidad Esperada Subjetiva (SEU).\nLas primeras tres etapas se basan en un modelo de utilidad esperada subjetiva (SEU, por sus siglas en inglés) que se utiliza para calcular la utilidad de cada opción. La utilidad se calcula al ponderar el valor de cada acción o decisión por la probabilidad subjetiva de que ocurra, este factor de ponderación igualmente se puede interpretar como la cantidad de atención que se le da a cada opción.\nPara ejemplificar el modelo se supondrá que se tienen dos opciones las cuales pueden conllevar a dos resultados diferentes cada uno con cierta probabilidad y cada posible resultado tendrá un valor. Se denotará las distintas acciones como A y B, sus posibles resultados como subíndices 1 y 2, y las probabilidades subjetivas de que ocurran como \\(w(S_1)\\) y \\(w(S_2)\\), respectivamente. La utilidad de cada resultado se denotará como \\(u(A_1)\\), \\(u(A_2)\\), \\(u(B_1)\\) y \\(u(B_2)\\). Entonces el utilidad esperada subjetiva (\\(v\\)) de cada elección se puede expresar como:\n\\[\\begin{equation}\n\\begin{aligned}\nv_A = w(S_1) \\cdot u(A_1) +  w(S_2) \\cdot u(A_2)\\\\\nv_B = w(S_1) \\cdot u(B_1) +  w(S_2) \\cdot u(B_2)\n\\end{aligned}\n\\end{equation}\\]\nLa elección entre las dos opciones se basa en la diferencia de utilidad entre las dos opciones, es decir, la opción con mayor utilidad esperada subjetiva será la elegida, matemáticamente esto se expresa como una diferencia de utilidad:\n\\[\nd = v_A - v_B\n\\]\nY la regla para decidir es:\n\\[\\begin{equation}\nDecisión =\n\\begin{cases}\nA & \\text{si } d &gt; 0\\\\\nB & \\text{si } d &lt; 0\\\\\n\\text{Indiferencia} & \\text{si } d = 0\n\\end{cases}\n\\end{equation}\\]\n\n\nUtilidad Esperada Subjetiva Aleatoria (RSEU).\nEl modelo SEU supone que la atención a cada opción es igual para cada presentación del mismo par de opciones. El modelo RSEU relaja esta suposición y permite que la atención a cada opción varíe en cada presentación, la diferencia \\(d\\) en cada ensayo es aleatoria y es llamada valencia diferencial (valence difference). En este modelo el peso atencional \\(W(S_i)\\) es una variable aleatoria que varía de ensayo a ensayo, representando así la flutuación en la atención.\nComo consecuencia la utilidad de cada opción se convierte en una variable aleatoria que a partir de ahora se le llamará como valencia (\\(V\\)). La cual se cálcula de firma idéntica al modelo SEU. La diferencia entre las valencias ahora se nombra como estado de preferencia \\(P\\), para cada ensayo y se cálcula como:\n\\[\nP = V_A - V_B = d + \\epsilon\n\\]\nDonde \\(\\epsilon\\) es un término de ruido que sigue una distribución de probabilidad, este término es el que permite que la elección varíe en cada ensayo. Por tanto, la probabilidad de elegir A sobre B se puede expresar como:\n\\[\nPr(A, B) = Pr(P &gt; 0) = Pr(\\epsilon &gt; -d)\n\\]\nEn muchos modelos de este estilo se supone que \\(\\epsilon\\) sigue una distribución normal con media 0 y varianza \\(\\sigma^2\\). Por tanto, la probabilidad de elección se puede expresar como:\n\\[\nPr(A, B) = Pr(\\epsilon &gt; -d) = \\Phi\\left(\\frac{d}{\\sigma}\\right)\n\\]\nDonde \\(\\Phi\\) es la función de distribución acumulada de la distribución normal estándar. La cuál se puede visualizar como la siguiente figura:\n\n\nCódigo\nimport numpy as np\nimport scipy.stats as stats\nimport plotly.express as px\nimport plotly.io as pio\nimport pandas as pd\n\npio.renderers.default = \"notebook\"\n\n\nx = np.linspace(-5, 5, 1000)\ny = stats.norm.cdf(x, 0, 1)\n\ndf = pd.DataFrame({'x': x, 'y': y})\nfig = px.line(df, x=\"x\", y=\"y\", title=\"Función de distribución acumulada de la distribución normal estándar\")\n\nfig.show()",
    "crumbs": [
      "Modelos de Difusión",
      "Modelo Decision Field Theory de Busemeyer"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca",
    "section": "",
    "text": "Lab25  es un laboratorio, localizado en la Facultad de Psicología de la Universidad Nacional Autónoma de México, cuyo objeto de estudio es el comportamiento adaptable, el aprendizaje y la decisión. Nuestra principal meta es contribuir a un mejor entendimiento del comportamiento de agentes humanos y no humanos como una respuesta adaptativa a la estructura estadística de su entorno. Actualmente, los temas de investigación que se llevan a cabo incluyen: \n | Aprendizaje por Refuerzo | Modelos Computacionales de Ansiedad | Elección Intertemporal | Elección Multiatributo | Causalidad | Teoría de Juegos | Adaptación a cambios en la condición de refuerzo | Modelamiento Bayesiano | Planeación | Generalización en Modelos de Refuerzo |",
    "crumbs": [
      "Acerca"
    ]
  }
]